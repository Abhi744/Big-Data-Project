{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"train.csv\").map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1).map(lambda line: (line[0]+ \"$%$\" +line[5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248576\n",
      "800000\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4', 50), ('0', 50)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()\n",
    "\n",
    "reduced_data_1 = data.filter(lambda x: x[0]=='4')\n",
    "reduced_data_0 = data.filter(lambda x: x[0]=='0')\n",
    "print(reduced_data_1.count())\n",
    "print(reduced_data_0.count())\n",
    "# zero_data = reduced_data_0.takeSample(False,248576,seed=None)\n",
    "# one_data = reduced_data_1.takeSample(False,248576,seed=None)\n",
    "zero_data = reduced_data_0.takeSample(False,50,seed=None)\n",
    "one_data = reduced_data_1.takeSample(False,50,seed=None)\n",
    "\n",
    "all_data = sc.parallelize(zero_data).union(sc.parallelize(one_data))\n",
    "\n",
    "labels = all_data.map(lambda x: x.split(\"$%$\")[0])\n",
    "print(labels.count())\n",
    "x = labels.map(lambda x: (x,1)).reduceByKey(add)\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_(tweet):\n",
    "    tweet_low2 = str(tweet).lower()\n",
    "    tweet_low3 = tweet_low2.replace(r\"\\n\",\"\")\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?$%^&*_~'''\n",
    "    tweet = \"\"\n",
    "    tweet_low = tweet_low3\n",
    "    if(tweet_low[0:4]==\"b'rt\"):\n",
    "        filtered_tweet = tweet_low[4:]\n",
    "    else:\n",
    "        filtered_tweet = tweet_low[2:]\n",
    "    for e in filtered_tweet:\n",
    "        if e not in punctuations:\n",
    "            tweet+=e\n",
    "    tweet_list = tweet.split(\" \")\n",
    "    tweet = \"\"\n",
    "    for e in tweet_list:\n",
    "        if(e!=\"\"):\n",
    "            if e[0]!='@' and e[0]!='#' and e[0:5]!=\"https\" and e[0:4]!=\"http\":\n",
    "                if(tweet!=\"\"):\n",
    "                    tweet += e + \" \"\n",
    "                else:\n",
    "                    tweet = e + \" \"\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    return tokens\n",
    "\n",
    "data_top_100_rdd = all_data\n",
    "texts = data_top_100_rdd.map(lambda x: x.split(\"$%$\")[1])\n",
    "# labels = data_top_100_rdd.map(lambda x: x.split(\"$%$\")[0])\n",
    "\n",
    "clean_texts=texts.map(lambda x: tokenize_(x))\n",
    "all_tweets = np.array(clean_texts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText([x for x in all_tweets], min_count=1,size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "def buildWordVector(tokens):\n",
    "    size=10\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        vec += model.wv[word].reshape((1, size))\n",
    "        count += 1.\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "all_tweets_ft = clean_texts.map(lambda x :buildWordVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(x):\n",
    "    if(int(x)==4):\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "def join_labels(x,all_labels,all_data_2):\n",
    "    index = x[1]\n",
    "    data_point = np.concatenate((all_labels[index],np.array(all_data_2[index])),axis=1)\n",
    "    return data_point\n",
    "\n",
    "labels2 = labels.map(change_label)\n",
    "all_labels = [[[l]] for l in np.array(labels2.collect())]\n",
    "all_data_2 = np.array(all_tweets_ft.collect())\n",
    "index_labels = labels2.zipWithIndex()\n",
    "\n",
    "all_data_labeled = index_labels.map(lambda x : join_labels(x,all_labels,all_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# all_data_labeled.first()\n",
    "\n",
    "tr,te = all_data_labeled.randomSplit([0.8, 0.2])\n",
    "train = tr.map(lambda x:x[0][1:])\n",
    "test = te.map(lambda x:x[0][1:])\n",
    "train_labels = np.array(tr.map(lambda x: x[0][0]).collect())\n",
    "test_labels = np.array(te.map(lambda x: x[0][0]).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parsing(x):\n",
    "    y = np.array(x[0])\n",
    "    return LabeledPoint(int(y[0]), y[1:])\n",
    "\n",
    "parse_tr = tr.map(parsing)\n",
    "parse_te = te.map(parsing)\n",
    "# parse.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "dtmodel = DecisionTree.trainClassifier(parse_tr, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "svmmodel = SVMWithSGD.train(parse_tr, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "lrmodel = LogisticRegressionWithLBFGS.train(parse_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "rfmodel = RandomForest.trainClassifier(parse_tr, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=4, featureSubsetStrategy=\"auto\" ,impurity='gini', maxDepth=4, maxBins=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "gbmodel = GradientBoostedTrees.trainClassifier(parse_tr,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1]), array([1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.]), array([1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.]), array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1]), array([0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "    \n",
    "predictions.append(np.array(svmmodel.predict(test).collect()))\n",
    "predictions.append(np.array(gbmodel.predict(test).collect()))\n",
    "predictions.append(np.array(rfmodel.predict(test).collect()))\n",
    "predictions.append(np.array(lrmodel.predict(test).collect()))\n",
    "predictions.append(np.array(dtmodel.predict(test).collect()))\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_np = np.array(predictions)\n",
    "cols = []\n",
    "for i in range(len(predictions_np[0])):\n",
    "    cols.append(predictions_np[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[435] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_(x):\n",
    "    return np.sum(x)\n",
    "count_ones = sc.parallelize(cols).map(lambda x: sum_(x))\n",
    "\n",
    "print(count_ones)\n",
    "def give_label(x):\n",
    "    if(int(x)>2):\n",
    "        return 1\n",
    "    return 0\n",
    "final_predictions = count_ones.map(lambda x: give_label(x))\n",
    "final_predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tp=0\n",
    "fp=0\n",
    "tn=0\n",
    "fn=0\n",
    "acc=0\n",
    "dd = np.array(final_predictions.collect())\n",
    "for i in range(len(dd)):\n",
    "    if(dd[i]==test_labels[i]):\n",
    "        acc+=1\n",
    "        if(dd[i]==0):\n",
    "            tp+=1\n",
    "        else:\n",
    "            tn+=1\n",
    "    else:\n",
    "        if(dd[i]==0):\n",
    "            fn+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "print(acc*100.0/len(dd))\n",
    "print(\"\")\n",
    "print(tp/(tp+fn))\n",
    "print(tn/(tn+fp))\n",
    "print(fp/(fp+tn))\n",
    "print(fn/(fn+tp))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
